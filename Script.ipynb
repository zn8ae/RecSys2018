{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpd = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import codecs\n",
    "import datetime\n",
    "\n",
    "\n",
    "pretty = True\n",
    "compact = False\n",
    "cache = {}\n",
    "\n",
    "def get_playlist(pid):\n",
    "    if pid >=0 and pid < 1000000:\n",
    "        low = 1000 * int(pid / 1000)\n",
    "        high = low + 999\n",
    "        offset = pid - low\n",
    "        path = \"./mpd.v1/data/mpd.slice.\" + str(low) + '-' + str(high) + \".json\"\n",
    "        if not path in cache:\n",
    "            f = codecs.open(path, 'r', 'utf-8')\n",
    "            js = f.read()\n",
    "            f.close()\n",
    "            playlist = json.loads(js)\n",
    "            cache[path] = playlist\n",
    "\n",
    "        playlist = cache[path]['playlists'][offset]\n",
    "        return playlist\n",
    "\n",
    "def get_playlists_in_range(start, end):\n",
    "    pls = []\n",
    "    try:\n",
    "        istart = int(start)\n",
    "        iend = int(end)\n",
    "        if istart <= iend and istart >= 0 and iend <= 1000000:\n",
    "            for pid in xrange(istart, iend):\n",
    "                pls.append(get_playlist(pid))\n",
    "    except:\n",
    "        raise\n",
    "        print \"bad pid\"\n",
    "    return pls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_playlists = 0\n",
    "total_tracks = 0\n",
    "tracks = set()\n",
    "artists = set()\n",
    "albums = set()\n",
    "titles = set()\n",
    "total_descriptions = 0\n",
    "ntitles = set()\n",
    "\n",
    "playlist_length_histogram = collections.Counter()\n",
    "num_followers_histogram = collections.Counter()\n",
    "artist_histogram = collections.Counter()\n",
    "track_histogram = collections.Counter()\n",
    "\n",
    "\n",
    "def process_playlist(playlist):\n",
    "    global total_playlists, total_tracks, total_descriptions\n",
    "\n",
    "    total_playlists += 1\n",
    "    # print playlist['playlist_id'], playlist['name']\n",
    "\n",
    "    if 'description' in playlist:\n",
    "        total_descriptions += 1\n",
    "\n",
    "    titles.add(playlist['name'])\n",
    "    nname = normalize_name(playlist['name'])\n",
    "    ntitles.add(nname)\n",
    "\n",
    "    playlist_length_histogram[playlist['num_tracks']] += 1\n",
    "    num_followers_histogram[playlist['num_followers']] += 1\n",
    "\n",
    "    for track in playlist['tracks']:\n",
    "        total_tracks += 1\n",
    "        albums.add(track['album_uri'])\n",
    "        tracks.add(track['track_uri'])\n",
    "        artists.add(track['artist_uri'])\n",
    "\n",
    "        full_name = track['track_name'] + \" by \" + track['artist_name']\n",
    "        artist_histogram[track['artist_name']] += 1\n",
    "        track_histogram[full_name] += 1\n",
    "\n",
    "def normalize_name(name):\n",
    "    name = name.lower()\n",
    "    name = re.sub(r\"[.,\\/#!$%\\^\\*;:{}=\\_`~()@]\", ' ', name)\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_summary():\n",
    "    print\n",
    "    print \"number of playlists\", total_playlists\n",
    "    print \"number of tracks\", total_tracks\n",
    "    print \"number of unique tracks\", len(tracks)\n",
    "    print \"number of unique albums\", len(albums)\n",
    "    print \"number of unique artists\", len(artists)\n",
    "    print \"number of unique titles\", len(titles)\n",
    "    print \"number of playlists with descriptions\", total_descriptions\n",
    "    print \"number of unique normalized titles\", len(ntitles)\n",
    "    print \"avg playlist length\", float(total_tracks) / total_playlists\n",
    "    \n",
    "    print\n",
    "    print \"top tracks\"\n",
    "    for track, count in track_histogram.most_common(20):\n",
    "        print \"%7d %s\" % (count, track)\n",
    "\n",
    "    print\n",
    "    print \"top artists\"\n",
    "    for artist, count in artist_histogram.most_common(20):\n",
    "        print \"%7d %s\" % (count, artist)\n",
    "\n",
    "    print\n",
    "    print \"playlist length histogram\"\n",
    "    for length, count in playlist_length_histogram.most_common(20):\n",
    "        print \"%7d %d\" % (count, length)\n",
    "\n",
    "    print\n",
    "    print \"num followers histogram\"\n",
    "    for followers, count in num_followers_histogram.most_common(20):\n",
    "        print \"%7d %d\" % (count, followers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri_track = {}\n",
    "def process_tracks(playlist):\n",
    "    global uri_track\n",
    "    document = []\n",
    "    \n",
    "    for track in playlist['tracks']:\n",
    "        uri = track['track_uri'].split(':')[2]\n",
    "        document.append(uri)\n",
    "        uri_track[uri] = track\n",
    "    \n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "l = get_playlists_in_range(0, 50000)\n",
    "l = np.array(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "l_train, l_test = train_test_split(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37500"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(l_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "#recommend for one list\n",
    "#go through training set, calculate score, stored in list\n",
    "#return top n ranked list sorted by score\n",
    "def calc_score(playlist):\n",
    "    dic = {}\n",
    "    tracks = playlist['tracks']\n",
    "    nname_test = []\n",
    "    for track in tracks:\n",
    "        nname = normalize_name(track['track_name'])\n",
    "        nname_test.append(nname)\n",
    "    total = len(nname_test)\n",
    "    \n",
    "    #for each playlist in training set, calculate score and add in result\n",
    "    for pl in l_train:\n",
    "        count = 0\n",
    "        tracks = pl['tracks']\n",
    "        for track in tracks:\n",
    "            nname = normalize_name(track['track_name'])\n",
    "            if nname in nname_test:\n",
    "                count+=1\n",
    "        #after finishing counting\n",
    "        dic[pl['pid']] = float(count)/total\n",
    "        \n",
    "    return dic, nname_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "dic, names = calc_score(l_test[0])\n",
    "result = sorted(dic.items(), key=operator.itemgetter(1))\n",
    "result.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3382, 0.4074074074074074)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(pt, scores, names, count):\n",
    "    result = []\n",
    "    for score in scores:\n",
    "        pid = score[0]\n",
    "        tracks = pt[pid]\n",
    "        for track in tracks:\n",
    "            nname = normalize_name(track['track_name'])\n",
    "            if nname not in names:\n",
    "                result.append(track)\n",
    "                if len(result) >= count:\n",
    "                    return result\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = recommend(pt, result, names, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pl in l_test:\n",
    "    dic, names = calc_score(pl)\n",
    "    result = sorted(dic.items(), key=operator.itemgetter(1))\n",
    "    result.reverse()\n",
    "    print (recommend(pt, result, names, 5))\n",
    "    print "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "- uri_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for plist in l_train:\n",
    "    documents.append(process_tracks(plist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-06 14:55:47,952 : INFO : collecting all words and their counts\n",
      "2018-05-06 14:55:47,954 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-05-06 14:55:48,212 : INFO : PROGRESS: at sentence #10000, processed 658827 words, keeping 171254 word types\n",
      "2018-05-06 14:55:48,497 : INFO : PROGRESS: at sentence #20000, processed 1327470 words, keeping 264797 word types\n",
      "2018-05-06 14:55:48,770 : INFO : PROGRESS: at sentence #30000, processed 2001779 words, keeping 339603 word types\n",
      "2018-05-06 14:55:48,986 : INFO : collected 387543 word types from a corpus of 2507932 raw words and 37500 sentences\n",
      "2018-05-06 14:55:48,987 : INFO : Loading a fresh vocabulary\n",
      "2018-05-06 14:55:49,896 : INFO : min_count=4 retains 80237 unique words (20% of original 387543, drops 307306)\n",
      "2018-05-06 14:55:49,897 : INFO : min_count=4 leaves 2093859 word corpus (83% of original 2507932, drops 414073)\n",
      "2018-05-06 14:55:50,158 : INFO : deleting the raw counts dictionary of 387543 items\n",
      "2018-05-06 14:55:50,171 : INFO : sample=0.001 downsamples 0 most-common words\n",
      "2018-05-06 14:55:50,172 : INFO : downsampling leaves estimated 2093859 word corpus (100.0% of prior 2093859)\n",
      "2018-05-06 14:55:50,407 : INFO : estimated required memory for 80237 words and 40 dimensions: 65794340 bytes\n",
      "2018-05-06 14:55:50,408 : INFO : resetting layer weights\n",
      "2018-05-06 14:55:51,099 : INFO : training model with 3 workers on 80237 vocabulary and 40 features, using sg=1 hs=0 sample=0.001 negative=5 window=8\n",
      "2018-05-06 14:55:52,130 : INFO : EPOCH 1 - PROGRESS: at 15.04% examples, 298702 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:55:53,140 : INFO : EPOCH 1 - PROGRESS: at 28.13% examples, 284393 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:55:54,171 : INFO : EPOCH 1 - PROGRESS: at 42.86% examples, 289067 words/s, in_qsize 5, out_qsize 1\n",
      "2018-05-06 14:55:55,174 : INFO : EPOCH 1 - PROGRESS: at 57.86% examples, 295591 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:55:56,177 : INFO : EPOCH 1 - PROGRESS: at 71.91% examples, 295740 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:55:57,212 : INFO : EPOCH 1 - PROGRESS: at 86.60% examples, 295943 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:55:58,033 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-06 14:55:58,039 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-06 14:55:58,072 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-06 14:55:58,072 : INFO : EPOCH - 1 : training on 2507932 raw words (2093859 effective words) took 7.0s, 300388 effective words/s\n",
      "2018-05-06 14:55:59,092 : INFO : EPOCH 2 - PROGRESS: at 15.36% examples, 310511 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:00,105 : INFO : EPOCH 2 - PROGRESS: at 31.41% examples, 318861 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:01,143 : INFO : EPOCH 2 - PROGRESS: at 47.28% examples, 319079 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:02,191 : INFO : EPOCH 2 - PROGRESS: at 63.02% examples, 318510 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:03,223 : INFO : EPOCH 2 - PROGRESS: at 78.24% examples, 317543 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:04,232 : INFO : EPOCH 2 - PROGRESS: at 93.60% examples, 318009 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:04,611 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-06 14:56:04,621 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-06 14:56:04,657 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-06 14:56:04,658 : INFO : EPOCH - 2 : training on 2507932 raw words (2093859 effective words) took 6.6s, 318092 effective words/s\n",
      "2018-05-06 14:56:05,692 : INFO : EPOCH 3 - PROGRESS: at 15.36% examples, 307793 words/s, in_qsize 5, out_qsize 1\n",
      "2018-05-06 14:56:06,697 : INFO : EPOCH 3 - PROGRESS: at 30.96% examples, 314731 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:07,711 : INFO : EPOCH 3 - PROGRESS: at 46.50% examples, 316041 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:08,730 : INFO : EPOCH 3 - PROGRESS: at 61.85% examples, 316265 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:09,750 : INFO : EPOCH 3 - PROGRESS: at 77.04% examples, 316491 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:10,770 : INFO : EPOCH 3 - PROGRESS: at 92.84% examples, 318142 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:11,191 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-06 14:56:11,206 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-06 14:56:11,239 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-06 14:56:11,240 : INFO : EPOCH - 3 : training on 2507932 raw words (2093859 effective words) took 6.6s, 318559 effective words/s\n",
      "2018-05-06 14:56:12,282 : INFO : EPOCH 4 - PROGRESS: at 15.36% examples, 306555 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:13,309 : INFO : EPOCH 4 - PROGRESS: at 30.96% examples, 310741 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:14,324 : INFO : EPOCH 4 - PROGRESS: at 46.50% examples, 313423 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:15,343 : INFO : EPOCH 4 - PROGRESS: at 62.23% examples, 316278 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:16,356 : INFO : EPOCH 4 - PROGRESS: at 77.41% examples, 316965 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:17,361 : INFO : EPOCH 4 - PROGRESS: at 92.84% examples, 317962 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:17,787 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-06 14:56:17,796 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-06 14:56:17,827 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-06 14:56:17,828 : INFO : EPOCH - 4 : training on 2507932 raw words (2093859 effective words) took 6.6s, 318492 effective words/s\n",
      "2018-05-06 14:56:18,849 : INFO : EPOCH 5 - PROGRESS: at 15.36% examples, 313000 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:19,867 : INFO : EPOCH 5 - PROGRESS: at 31.41% examples, 319527 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:20,868 : INFO : EPOCH 5 - PROGRESS: at 46.50% examples, 317919 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:21,877 : INFO : EPOCH 5 - PROGRESS: at 62.23% examples, 320586 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:22,878 : INFO : EPOCH 5 - PROGRESS: at 77.41% examples, 321151 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:23,882 : INFO : EPOCH 5 - PROGRESS: at 92.84% examples, 321457 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:24,313 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-06 14:56:24,321 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-06 14:56:24,359 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-06 14:56:24,360 : INFO : EPOCH - 5 : training on 2507932 raw words (2093859 effective words) took 6.5s, 321219 effective words/s\n",
      "2018-05-06 14:56:24,361 : INFO : training on a 12539660 raw words (10469295 effective words) took 33.3s, 314764 effective words/s\n",
      "2018-05-06 14:56:24,362 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2018-05-06 14:56:24,363 : INFO : training model with 3 workers on 80237 vocabulary and 40 features, using sg=1 hs=0 sample=0.001 negative=5 window=8\n",
      "2018-05-06 14:56:25,393 : INFO : EPOCH 1 - PROGRESS: at 12.61% examples, 252511 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:26,404 : INFO : EPOCH 1 - PROGRESS: at 25.74% examples, 261730 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:27,423 : INFO : EPOCH 1 - PROGRESS: at 38.95% examples, 264280 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:28,446 : INFO : EPOCH 1 - PROGRESS: at 52.06% examples, 265189 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:29,450 : INFO : EPOCH 1 - PROGRESS: at 64.98% examples, 266570 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:30,462 : INFO : EPOCH 1 - PROGRESS: at 77.41% examples, 265757 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:31,478 : INFO : EPOCH 1 - PROGRESS: at 89.73% examples, 263982 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:32,311 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-06 14:56:32,320 : INFO : worker thread finished; awaiting finish of 1 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-06 14:56:32,360 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-06 14:56:32,360 : INFO : EPOCH - 1 : training on 2507932 raw words (2093859 effective words) took 8.0s, 262278 effective words/s\n",
      "2018-05-06 14:56:33,384 : INFO : EPOCH 2 - PROGRESS: at 11.89% examples, 235022 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:34,400 : INFO : EPOCH 2 - PROGRESS: at 23.77% examples, 239894 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:35,434 : INFO : EPOCH 2 - PROGRESS: at 36.54% examples, 245943 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:36,450 : INFO : EPOCH 2 - PROGRESS: at 48.42% examples, 245746 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:37,462 : INFO : EPOCH 2 - PROGRESS: at 58.73% examples, 239768 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:38,506 : INFO : EPOCH 2 - PROGRESS: at 67.66% examples, 229634 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:39,592 : INFO : EPOCH 2 - PROGRESS: at 75.87% examples, 219029 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:40,635 : INFO : EPOCH 2 - PROGRESS: at 84.26% examples, 212616 words/s, in_qsize 6, out_qsize 1\n",
      "2018-05-06 14:56:41,682 : INFO : EPOCH 2 - PROGRESS: at 93.96% examples, 210941 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:42,166 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-06 14:56:42,185 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-06 14:56:42,226 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-06 14:56:42,227 : INFO : EPOCH - 2 : training on 2507932 raw words (2093859 effective words) took 9.9s, 212278 effective words/s\n",
      "2018-05-06 14:56:43,303 : INFO : EPOCH 3 - PROGRESS: at 12.61% examples, 238932 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:44,308 : INFO : EPOCH 3 - PROGRESS: at 24.58% examples, 243096 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:45,313 : INFO : EPOCH 3 - PROGRESS: at 36.54% examples, 244872 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:46,342 : INFO : EPOCH 3 - PROGRESS: at 48.42% examples, 244170 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:47,349 : INFO : EPOCH 3 - PROGRESS: at 60.33% examples, 244610 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:48,350 : INFO : EPOCH 3 - PROGRESS: at 71.91% examples, 245213 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:49,352 : INFO : EPOCH 3 - PROGRESS: at 83.44% examples, 244638 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:50,372 : INFO : EPOCH 3 - PROGRESS: at 95.12% examples, 244449 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:50,824 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-06 14:56:50,843 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-06 14:56:50,891 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-06 14:56:50,892 : INFO : EPOCH - 3 : training on 2507932 raw words (2093859 effective words) took 8.7s, 241711 effective words/s\n",
      "2018-05-06 14:56:51,916 : INFO : EPOCH 4 - PROGRESS: at 11.89% examples, 235003 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:52,951 : INFO : EPOCH 4 - PROGRESS: at 22.15% examples, 221891 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:53,951 : INFO : EPOCH 4 - PROGRESS: at 34.18% examples, 230642 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:54,960 : INFO : EPOCH 4 - PROGRESS: at 44.07% examples, 224417 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:56,041 : INFO : EPOCH 4 - PROGRESS: at 55.95% examples, 225857 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:57,067 : INFO : EPOCH 4 - PROGRESS: at 66.52% examples, 224483 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:56:58,146 : INFO : EPOCH 4 - PROGRESS: at 78.24% examples, 225367 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:59,201 : INFO : EPOCH 4 - PROGRESS: at 90.13% examples, 226738 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:56:59,948 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-06 14:56:59,962 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-06 14:57:00,004 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-06 14:57:00,005 : INFO : EPOCH - 4 : training on 2507932 raw words (2093859 effective words) took 9.1s, 229832 effective words/s\n",
      "2018-05-06 14:57:01,025 : INFO : EPOCH 5 - PROGRESS: at 11.89% examples, 235838 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:57:02,059 : INFO : EPOCH 5 - PROGRESS: at 23.37% examples, 234342 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:57:03,098 : INFO : EPOCH 5 - PROGRESS: at 35.74% examples, 239589 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:57:04,121 : INFO : EPOCH 5 - PROGRESS: at 46.50% examples, 233898 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:05,186 : INFO : EPOCH 5 - PROGRESS: at 55.95% examples, 225259 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:57:06,186 : INFO : EPOCH 5 - PROGRESS: at 67.30% examples, 226973 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:07,189 : INFO : EPOCH 5 - PROGRESS: at 79.03% examples, 229952 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:08,191 : INFO : EPOCH 5 - PROGRESS: at 90.89% examples, 232263 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:57:08,888 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-06 14:57:08,899 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-06 14:57:08,942 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-06 14:57:08,943 : INFO : EPOCH - 5 : training on 2507932 raw words (2093859 effective words) took 8.9s, 234331 effective words/s\n",
      "2018-05-06 14:57:10,010 : INFO : EPOCH 6 - PROGRESS: at 12.61% examples, 242432 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:11,034 : INFO : EPOCH 6 - PROGRESS: at 23.37% examples, 230868 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:12,076 : INFO : EPOCH 6 - PROGRESS: at 35.35% examples, 233703 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:13,083 : INFO : EPOCH 6 - PROGRESS: at 45.25% examples, 226905 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:57:14,085 : INFO : EPOCH 6 - PROGRESS: at 57.86% examples, 234521 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:15,135 : INFO : EPOCH 6 - PROGRESS: at 69.95% examples, 236121 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:16,139 : INFO : EPOCH 6 - PROGRESS: at 81.45% examples, 236657 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:57:17,164 : INFO : EPOCH 6 - PROGRESS: at 93.96% examples, 239388 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:17,585 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-06 14:57:17,590 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-06 14:57:17,637 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-06 14:57:17,638 : INFO : EPOCH - 6 : training on 2507932 raw words (2093859 effective words) took 8.7s, 241070 effective words/s\n",
      "2018-05-06 14:57:18,681 : INFO : EPOCH 7 - PROGRESS: at 12.61% examples, 246459 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:19,704 : INFO : EPOCH 7 - PROGRESS: at 25.74% examples, 257143 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:57:20,744 : INFO : EPOCH 7 - PROGRESS: at 38.11% examples, 253972 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:21,751 : INFO : EPOCH 7 - PROGRESS: at 49.61% examples, 250384 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:22,829 : INFO : EPOCH 7 - PROGRESS: at 60.71% examples, 242882 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:23,869 : INFO : EPOCH 7 - PROGRESS: at 71.14% examples, 238287 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:24,925 : INFO : EPOCH 7 - PROGRESS: at 83.03% examples, 238091 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:25,973 : INFO : EPOCH 7 - PROGRESS: at 94.75% examples, 237893 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:57:26,348 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-06 14:57:26,362 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-06 14:57:26,407 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-06 14:57:26,408 : INFO : EPOCH - 7 : training on 2507932 raw words (2093859 effective words) took 8.8s, 238821 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-06 14:57:27,423 : INFO : EPOCH 8 - PROGRESS: at 11.89% examples, 237125 words/s, in_qsize 6, out_qsize 1\n",
      "2018-05-06 14:57:28,438 : INFO : EPOCH 8 - PROGRESS: at 23.37% examples, 237112 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:57:29,488 : INFO : EPOCH 8 - PROGRESS: at 35.74% examples, 240593 words/s, in_qsize 6, out_qsize 1\n",
      "2018-05-06 14:57:30,525 : INFO : EPOCH 8 - PROGRESS: at 47.61% examples, 239938 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:31,545 : INFO : EPOCH 8 - PROGRESS: at 59.57% examples, 240796 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:32,616 : INFO : EPOCH 8 - PROGRESS: at 72.32% examples, 243159 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:33,617 : INFO : EPOCH 8 - PROGRESS: at 85.05% examples, 246301 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:34,678 : INFO : EPOCH 8 - PROGRESS: at 97.05% examples, 245864 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:34,859 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-06 14:57:34,866 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-06 14:57:34,905 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-06 14:57:34,906 : INFO : EPOCH - 8 : training on 2507932 raw words (2093859 effective words) took 8.5s, 246477 effective words/s\n",
      "2018-05-06 14:57:35,968 : INFO : EPOCH 9 - PROGRESS: at 12.61% examples, 243393 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:37,006 : INFO : EPOCH 9 - PROGRESS: at 25.74% examples, 253628 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:38,015 : INFO : EPOCH 9 - PROGRESS: at 38.11% examples, 254221 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:57:39,043 : INFO : EPOCH 9 - PROGRESS: at 51.23% examples, 257328 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:40,060 : INFO : EPOCH 9 - PROGRESS: at 64.22% examples, 259556 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:57:41,105 : INFO : EPOCH 9 - PROGRESS: at 77.04% examples, 259854 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:42,148 : INFO : EPOCH 9 - PROGRESS: at 90.13% examples, 260332 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:57:42,895 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-06 14:57:42,903 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-06 14:57:42,946 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-06 14:57:42,947 : INFO : EPOCH - 9 : training on 2507932 raw words (2093859 effective words) took 8.0s, 260672 effective words/s\n",
      "2018-05-06 14:57:43,958 : INFO : EPOCH 10 - PROGRESS: at 9.77% examples, 196294 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:44,963 : INFO : EPOCH 10 - PROGRESS: at 20.09% examples, 205924 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:57:45,989 : INFO : EPOCH 10 - PROGRESS: at 32.19% examples, 218447 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:57:46,995 : INFO : EPOCH 10 - PROGRESS: at 44.07% examples, 225511 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:57:48,013 : INFO : EPOCH 10 - PROGRESS: at 55.95% examples, 229550 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:49,048 : INFO : EPOCH 10 - PROGRESS: at 67.66% examples, 231315 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:57:50,050 : INFO : EPOCH 10 - PROGRESS: at 80.27% examples, 236080 words/s, in_qsize 6, out_qsize 0\n",
      "2018-05-06 14:57:51,071 : INFO : EPOCH 10 - PROGRESS: at 92.03% examples, 237049 words/s, in_qsize 5, out_qsize 0\n",
      "2018-05-06 14:57:51,733 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-06 14:57:51,749 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-06 14:57:51,792 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-06 14:57:51,793 : INFO : EPOCH - 10 : training on 2507932 raw words (2093859 effective words) took 8.8s, 236771 effective words/s\n",
      "2018-05-06 14:57:51,794 : INFO : training on a 25079320 raw words (20938590 effective words) took 87.4s, 239489 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20938590, 25079320)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec (documents, size=40, window=8, min_count=4, sg=1)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'17VegeBoHvMlIByrdu64KR'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_train[0]['tracks'][0]['track_uri'].split(\":\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'album_name': u'Elton John',\n",
       " u'album_uri': u'spotify:album:7dtLYwLOdYQa2S8Vjeuxci',\n",
       " u'artist_name': u'Elton John',\n",
       " u'artist_uri': u'spotify:artist:3PhoLpVuITZKcymswpck5b',\n",
       " u'duration_ms': 244226,\n",
       " u'pos': 0,\n",
       " u'track_name': u'Your Song',\n",
       " u'track_uri': u'spotify:track:17VegeBoHvMlIByrdu64KR'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l_train[0]['tracks'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-06 14:59:03,546 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    }
   ],
   "source": [
    "w1 = l_train[0]['tracks'][0]['track_uri'].split(\":\")[2]\n",
    "rec_list = model.wv.most_similar(positive=w1, topn = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rec_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Your Song By Elton John\n",
      "\n",
      "Tiny Dancer By Elton John\n",
      "similarity: 0.911042332649\n",
      "\n",
      "Goodbye Yellow Brick Road - Remastered 2014 By Elton John\n",
      "similarity: 0.904203534126\n",
      "\n",
      "Rocket Man (I Think It's Going To Be A Long Long Time) By Elton John\n",
      "similarity: 0.892361581326\n",
      "\n",
      "Candle In The Wind - Remastered 2014 By Elton John\n",
      "similarity: 0.880346298218\n",
      "\n",
      "Piano Man By Billy Joel\n",
      "similarity: 0.869542479515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Test: \" + uri_track[w1]['track_name'] + \" By \" + uri_track[w1]['artist_name']\n",
    "print\n",
    "for l in rec_list:\n",
    "    print uri_track[l[0]]['track_name'] + \" By \" + uri_track[l[0]]['artist_name']\n",
    "    print \"similarity: \" + str(l[1])\n",
    "    print\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "- Jaccard Similarity measures intersection over union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(pl):\n",
    "    \n",
    "    predicts = []\n",
    "    tracks = pl['tracks']\n",
    "    #withheld tracks 50/50\n",
    "    leng = len(tracks)\n",
    "    t_train, t_test = train_test_split(tracks,test_size=0.5)\n",
    "    #for the train 50%, predict x5\n",
    "    for t in t_train:\n",
    "        uri = t['track_uri'].split(\":\")[-1]\n",
    "        if uri not in model.wv.vocab.keys():\n",
    "            continue\n",
    "        res = model.wv.most_similar(positive=uri, topn=5)\n",
    "        for l in res:\n",
    "            predicts.append(l[0])\n",
    "    \n",
    "    len_res = len(predicts)\n",
    "    \n",
    "    compare = []\n",
    "    for t in t_test:\n",
    "        compare.append(t['track_uri'].split(\":\")[-1])\n",
    "    \n",
    "    return predicts, compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_distance(l1, l2):\n",
    "    intersection = 0\n",
    "    for i in l1:\n",
    "        if i in l2:\n",
    "            intersection +=1\n",
    "    union = (len(l1) + len(l2)) - intersection\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Jaccard Distance\n",
      "0.0578745372613\n"
     ]
    }
   ],
   "source": [
    "total = []\n",
    "for plist in l_test:\n",
    "    pred, comp = eval(plist)\n",
    "    dist = jaccard_distance(pred, comp)\n",
    "    total.append(dist)\n",
    "\n",
    "print \"Average Jaccard Distance\"\n",
    "print sum(total) / float(len(total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try it out for yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "track=[]\n",
    "target = [\"Moon River\", \"Molly\", \"Fly me to the moon\", \"Perfect\"]\n",
    "for i in uri_track:\n",
    "    if uri_track[i]['track_name'] in target:\n",
    "        track.append(uri_track[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: Perfect By Simple Plan\n",
      "\n",
      "7 Minutes In Heaven (Atavan Halen) By Fall Out Boy\n",
      "\n",
      "Addicted By Simple Plan\n",
      "\n",
      "One Day By Simple Plan\n",
      "\n",
      "Hum Hallelujah By Fall Out Boy\n",
      "\n",
      "Thank You By Simple Plan\n",
      "\n",
      "Grand Theft Autumn/Where Is Your Boy By Fall Out Boy\n",
      "\n",
      "Kelsey By Metro Station\n",
      "\n",
      "That's What You Get By Paramore\n",
      "\n",
      "Kids In Love By Mayday Parade\n",
      "\n",
      "Nice Guys Finish Last By Cobra Starship\n",
      "\n",
      "----------------\n",
      "input: Perfect By Ed Sheeran\n",
      "\n",
      "Happier By Ed Sheeran\n",
      "\n",
      "Dive By Ed Sheeran\n",
      "\n",
      "Hearts Don't Break Around Here By Ed Sheeran\n",
      "\n",
      "Supermarket Flowers By Ed Sheeran\n",
      "\n",
      "Save Myself By Ed Sheeran\n",
      "\n",
      "What Do I Know? By Ed Sheeran\n",
      "\n",
      "Idea of Her By Whitney Woerz\n",
      "\n",
      "Roses By MacKenzie Bourg\n",
      "\n",
      "Strip That Down - Acoustic By Liam Payne\n",
      "\n",
      "Ex By James TW\n",
      "\n",
      "----------------\n",
      "input: Perfect By One Direction\n",
      "\n",
      "Cough Syrup - The Voice Performance By Matthew Schuler\n",
      "\n",
      "Olivia By One Direction\n",
      "\n",
      "End of the Day By One Direction\n",
      "\n",
      "Drag Me Down By One Direction\n",
      "\n",
      "Wolves By One Direction\n",
      "\n",
      "Infinity By One Direction\n",
      "\n",
      "Walking in the Wind By One Direction\n",
      "\n",
      "Let's Be Birds By Jacob Whitesides\n",
      "\n",
      "Clouds By One Direction\n",
      "\n",
      "Grease (Is The Word) - From \"Grease Live! Music from the Television Event\" By Jessie J\n",
      "\n",
      "----------------\n",
      "input: Perfect By My Darkest Days\n",
      "\n",
      "Does Everybody In The World Have To Die By Hollywood Undead\n",
      "\n",
      "Hero - Red Pill Mix By Superchick\n",
      "\n",
      "Never Back Down By Nine Lashes\n",
      "\n",
      "Black & Blue By Sick Puppies\n",
      "\n",
      "Stick To Your Guns By Sick Puppies\n",
      "\n",
      "Anthem of the Lonely By Nine Lashes\n",
      "\n",
      "Paradise Lost By Hollywood Undead\n",
      "\n",
      "Breaking Inside - feat. Lzzy Hale of Halestorm [Bonus Track] By Shinedown\n",
      "\n",
      "The Bleeding By Five Finger Death Punch\n",
      "\n",
      "Warriors By Papa Roach\n",
      "\n",
      "----------------\n",
      "input: Perfect By Darius Rucker\n",
      "\n",
      "Voices By Chris Young\n",
      "\n",
      "Lookin' For A Good Time By Lady Antebellum\n",
      "\n",
      "A Man This Lonely By Brooks & Dunn\n",
      "\n",
      "Better Life By Keith Urban\n",
      "\n",
      "Baby I'm Right By Darius Rucker\n",
      "\n",
      "Unstoppable By Rascal Flatts\n",
      "\n",
      "Cool Thing By Rascal Flatts\n",
      "\n",
      "Show You off Tonight By Lee Brice\n",
      "\n",
      "Summer Girl By Leighton Meester\n",
      "\n",
      "Up! - Red Version By Shania Twain\n",
      "\n",
      "----------------\n",
      "input: Perfect By Leroy Sanchez\n",
      "\n",
      "Stitches By Conor Maynard\n",
      "\n",
      "Sorry By Halsey\n",
      "\n",
      "Suicide By James Arthur\n",
      "\n",
      "I Will Not Kiss You By Melting Oak\n",
      "\n",
      "Needed Me By Austin Awake\n",
      "\n",
      "DNA By Lia Marie Johnson\n",
      "\n",
      "Finally By James Arthur\n",
      "\n",
      "We Should Be Together By Pia Mia\n",
      "\n",
      "Thank You for the Broken Heart By J Rice\n",
      "\n",
      "Don't Let Me Down By Joy Williams\n",
      "\n",
      "----------------\n",
      "input: Perfect By Hedley\n",
      "\n",
      "Skip To The Good Part By He Is We\n",
      "\n",
      "Tell Me By He Is We\n",
      "\n",
      "Slow By Andy Grammer\n",
      "\n",
      "Stronger (Acoustic Version) By Jess Moskaluke\n",
      "\n",
      "How By Maroon 5\n",
      "\n",
      "You're Beautiful (Clean Version) By Chester See\n",
      "\n",
      "Amaranthine - Acoustic By Amaranthe\n",
      "\n",
      "Losing My Mind - Non-LP Version By Maroon 5\n",
      "\n",
      "Favorite Girl By The Icarus Account\n",
      "\n",
      "Think Of You By A Fine Frenzy\n",
      "\n",
      "----------------\n",
      "input: Molly By Lil Pump\n",
      "\n",
      "444+222 By Lil Uzi Vert\n",
      "\n",
      "D Rose By Lil Pump\n",
      "\n",
      "No Flag By London On Da Track\n",
      "\n",
      "Had To By Kevin Gates\n",
      "\n",
      "For Real By Lil Uzi Vert\n",
      "\n",
      "Minute By NAV\n",
      "\n",
      "Lil Pump By Lil Pump\n",
      "\n",
      "Flex Season By Yung Gravy\n",
      "\n",
      "Flex Like Ouu By Lil Pump\n",
      "\n",
      "Boss By Lil Pump\n",
      "\n",
      "----------------\n",
      "input: Perfect By Selena Gomez\n",
      "\n",
      "Camouflage By Selena Gomez\n",
      "\n",
      "I Would By Justin Bieber\n",
      "\n",
      "Revival By Selena Gomez\n",
      "\n",
      "See You Around By Chris Brown\n",
      "\n",
      "Paper Hearts By Tori Kelly\n",
      "\n",
      "Memories By Shawn Mendes\n",
      "\n",
      "Kid In Love - Live By Shawn Mendes\n",
      "\n",
      "Like My Daddy By Empire Cast\n",
      "\n",
      "Forget Forever - ST£FAN Remix By Selena Gomez\n",
      "\n",
      "In Case By Demi Lovato\n",
      "\n",
      "----------------\n",
      "input: Molly By Tyga\n",
      "\n",
      "Bounce It By Juicy J\n",
      "\n",
      "All Gold Everything By Trinidad James\n",
      "\n",
      "Big Spender (Cover) By Bobby JaGGerJacK\n",
      "\n",
      "Bugatti By Ace Hood\n",
      "\n",
      "We Still In This Bitch (feat. T.I. and Juicy J) By B.o.B\n",
      "\n",
      "Dope By Tyga\n",
      "\n",
      "I Wish You Would By DJ Khaled\n",
      "\n",
      "Dance (A$$) By Big Sean\n",
      "\n",
      "Stay Fly - Remix - Explicit By Three 6 Mafia\n",
      "\n",
      "Hijack By Tyga\n",
      "\n",
      "----------------\n",
      "input: Moon River By Gypsy Flamenco Masters\n",
      "\n",
      "False Alarms By Josh Groban\n",
      "\n",
      "Below The Line By Josh Groban\n",
      "\n",
      "Sleepsong By Rolf Løvland\n",
      "\n",
      "Coming Home, Pt. II By Jackie Evancho\n",
      "\n",
      "Now Or Never By Josh Groban\n",
      "\n",
      "E Ho`i I Ka Pili By Keali'i Reichel\n",
      "\n",
      "I'm Yours By Vitamin String Quartet\n",
      "\n",
      "Dry Hands By C418\n",
      "\n",
      "モーツァルト - Sonata No. 16 C major (Sonata facile) , KV 545 (1788) 1 allegro By ピアノ\n",
      "\n",
      "Ave Maria, D. 839, Op. 52, No. 6 (Ellens Gesang III from \"Ein Fräulein vom See\") [Arr. for Cello and Piano] By Franz Schubert\n",
      "\n",
      "----------------\n",
      "input: Perfect By The Smashing Pumpkins\n",
      "\n",
      "Make Yourself By Incubus\n",
      "\n",
      "New Age Girl By Deadeye Dick\n",
      "\n",
      "Promise By Eve 6\n",
      "\n",
      "Supernova By Liz Phair\n",
      "\n",
      "Father Of Mine - 2004 Digital Remaster By Everclear\n",
      "\n",
      "Then The Morning Comes By Smash Mouth\n",
      "\n",
      "End It On This By No Doubt\n",
      "\n",
      "6th Avenue Heartache By The Wallflowers\n",
      "\n",
      "Praise You By Fatboy Slim\n",
      "\n",
      "Jimmy Olsen's Blues By Spin Doctors\n",
      "\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "damn = []\n",
    "for i in track:\n",
    "    w1 = i['track_uri'].split(\":\")[-1]\n",
    "    if w1 not in model.wv.vocab.keys():\n",
    "            continue\n",
    "    res = model.wv.most_similar(positive=w1)\n",
    "    print \"input: \" + i['track_name'] + \" By \" + i[\"artist_name\"]\n",
    "    print\n",
    "    for l in res:\n",
    "        print uri_track[l[0]]['track_name'] + \" By \" + uri_track[l[0]]['artist_name']\n",
    "        print \n",
    "    print \"----------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
